{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83063aeb",
   "metadata": {},
   "source": [
    "# Libraries \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c77d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from IPython.core.display import display\n",
    "from librosa.core import istft\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3370c12f",
   "metadata": {},
   "source": [
    "# Directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891aa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Directory to Audio files, Mention your own before running the code segment\n",
    "##Assigning directory for each train and test set \n",
    "\n",
    "train_audio='/Users/divyansh/Downloads/nsynth-train/audio/'\n",
    "test_audio='/Users/divyansh/Downloads/nsynth-test/audio/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Directory to json files, Mention your own before running the code segment\n",
    "##Reading Jsons files for both Train and Test dataframes\n",
    "\n",
    "##Training dataframe with features from json file\n",
    "df_train=pd.read_json(path_or_buf='/Users/divyansh/Downloads/nsynth-train/examples.json',orient='index')\n",
    "##Testing dataframe with features from json file\n",
    "df_test=pd.read_json(path_or_buf='/Users/divyansh/Downloads/nsynth-test/examples.json',orient='index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d7c66",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e41b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02537cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Showing the imbalanced distribution for instrument family in train dataframe\n",
    "\n",
    "df_train['instrument_family'].value_counts().reindex(np.arange(0,11,1)).plot.bar()\n",
    "plt.title('Instrument Family Distribution for Training')\n",
    "plt.xlabel('Instrument')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Counting each instrument family examples for the Training DataFrame\n",
    "\n",
    "classes= df_train['instrument_family'].value_counts(ascending=True)\n",
    "classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sample n files\n",
    "\n",
    "df_train=df_train.groupby('instrument_family', as_index=False, #group by instrument family\n",
    "                                 group_keys=False).apply(lambda df: df.sample(400))\n",
    "\n",
    "##Dropping the 9th instrument family member cause of it's negligible contribution \n",
    "\n",
    "df_train= df_train[df_train['instrument_family']!=9]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sampling the instrument family distribution for training \n",
    "\n",
    "df_train['instrument_family'].value_counts().reindex(np.arange(0,len(classes)+1,1)).plot.bar()\n",
    "plt.title('Sampled Instrument Family Distribution for Training')\n",
    "plt.xlabel('Instrument Family')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eefdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##View the train dataframe after sampling\n",
    "\n",
    "df_train['instrument_family'].value_counts(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236af0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting the train dataframe into a list\n",
    "\n",
    "trainfile=df_train.index.tolist()\n",
    "\n",
    "\n",
    "##Saving the train list to a pickle file\n",
    "\n",
    "with open('/Users/divyansh/Downloads/Train-Test Data/trainfile.pickle','wb') as f:\n",
    "    pickle.dump(trainfile,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test Dataframe\n",
    "\n",
    "##Sampling the instrument family distribution for testing \n",
    "\n",
    "df_test['instrument_family'].value_counts().reindex(np.arange(0,11,1)).plot.bar()\n",
    "plt.title('Sampled Instrument Family Distribution for Testing')\n",
    "plt.xlabel('Instrument Family')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca999a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting the test dataframe into a list\n",
    "testfile=df_test.index.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving the test file to a pickle file\n",
    "\n",
    "with open('/Users/divyansh/Downloads/Train-Test Data/testfile.pickle','wb') as f:\n",
    "    pickle.dump(testfile,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88081c55",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction and Visualization\n",
    "\n",
    "def features_extract(file):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defining function that takes in a file and returns features in an array\n",
    "    \"\"\"\n",
    "    \n",
    "    #Getting a Wave Representation for the audio file\n",
    "    \n",
    "    w, sr = librosa.load(file)\n",
    "    w,index =librosa.effects.trim(w)\n",
    "        \n",
    "    #Graphs and Values resulting from Freq Domain and dB powered are just for experimental purposes\n",
    "    #Just to realize how we are actually doing the process mathematically in terms of visual graphs\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    ##Determine if the instrument is Harmonic or Percussive \n",
    "    \n",
    "    w_harmonic, w_percussive = librosa.effects.hpss(w)\n",
    "    if np.mean(w_harmonic)>np.mean(w_percussive):\n",
    "        harmonic=1\n",
    "    else:\n",
    "        harmonic=0\n",
    "    \n",
    "    ##Mel Scaled Spectrogram\n",
    "\n",
    "    spectrogram= librosa.feature.melspectrogram(w,sr=sr, hop_length=512, n_mels=128,fmax=8000)\n",
    "    \n",
    "    #librosa.display.specshow(spectrogram, sr=sr, hop_length=512, x_axis='time',y_axis='mel')\n",
    "    #plt.title('Mel Scaled')\n",
    "    #plt.show()\n",
    "    \n",
    "    ##Log powered spectrum\n",
    "    db_melscaled= librosa.power_to_db(spectrogram,ref=np.max)\n",
    "    \n",
    "    #librosa.display.specshow(db_melscaled,sr=sr,hop_length=512,x_axis='time',y_axis='mel')\n",
    "    #plt.colorbar(format='%+2.0f dB')\n",
    "    #plt.tight_layout()\n",
    "    #plt.title('Decibel')\n",
    "    #plt.show()\n",
    "    \n",
    "    #We take average for each feature to fix MxN array into a table\n",
    "    \n",
    "    #Temporal Averaging\n",
    "    spectrogram=np.mean(spectrogram,axis=1)\n",
    "    \n",
    "    #print(\"Shape for Spectrogram\",spectrogram.shape)\n",
    "\n",
    "    \n",
    "    ##Mel-Frequency Cepstral Coefficients (MFCCs)\n",
    "    dbmelscaled = istft(db_melscaled)\n",
    "    mfcc = librosa.feature.mfcc(dbmelscaled, sr=sr, n_mfcc=13)\n",
    "    #librosa.display.specshow(mfcc,x_axis='time')\n",
    "    #plt.title('Mfcc')\n",
    "    #plt.show()\n",
    "    \n",
    "    ##Temporal Averaging\n",
    "    mfcc=np.mean(mfcc,axis=1)\n",
    "    #print(mfcc.shape)\n",
    "    \n",
    "    ##Compute Chroma Energy\n",
    "    chroma = librosa.feature.chroma_cens(w, sr=sr)\n",
    "    #librosa.display.specshow(chroma,x_axis='time',y_axis='mel')\n",
    "    #plt.title('Chroma')\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    #Temporally Average Chroma\n",
    "    chroma = np.mean(chroma, axis = 1)\n",
    "    #print(\"Chroma points shape\",chroma.shape)\n",
    "    \n",
    "    ##Compute Spectral Contrast\n",
    "    contrast = librosa.feature.spectral_contrast(w, sr=sr)\n",
    "    \n",
    "    #librosa.display.specshow(contrast,x_axis='time')\n",
    "    #plt.title('Contrast')\n",
    "    #plt.ylabel('Frequency bands')\n",
    "    #plt.colorbar(format='%+2.0f dB')\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    #Temporally Average Contrast\n",
    "    contrast = np.mean(contrast, axis= 1)\n",
    "    \n",
    "    #print(\"Contrast\",contrast.shape)\n",
    "    \n",
    "    #Passing the specific features we are gonna be putting into our train/test sets\n",
    "    return [harmonic,spectrogram,mfcc,chroma,contrast]\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ad7e3",
   "metadata": {},
   "source": [
    "# Instrument Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ece858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrument_class(file):\n",
    "    \"\"\"\n",
    "    Function that takes in a file and returns label i.e. instrument based on naming convention\n",
    "    \"\"\"\n",
    "    \n",
    "    ##All the 10 instruments serving as labels in the dataset\n",
    "    labels=['brass','bass','flute','guitar','keyboard','mallet','organ','reed','string','synth_lead','vocal']\n",
    "    \n",
    "    for name in labels:\n",
    "        if name in file:\n",
    "            return labels.index(name)\n",
    "        else:\n",
    "            None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaad3f",
   "metadata": {},
   "source": [
    "# Train Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2261c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training dataframe with audio .wav files\n",
    "\n",
    "\n",
    "#Dictionary for storing features in the train file\n",
    "traindict = {}\n",
    "\n",
    "#Running a loop over each file in the list\n",
    "#Storing the features extracted from the audio files to the train dict\n",
    "#'train_audio' being the directory to audio files\n",
    "\n",
    "for file in trainfile:\n",
    "    features = features_extract(train_audio +file+ '.wav') ##specifying the directory and .wav\n",
    "    traindict[file]= features\n",
    "\n",
    "#Results of the specific audio files corresponding to json files in the filename_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51e1dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convering the dictionary into DataFrame\n",
    "ftrain= pd.DataFrame.from_dict(traindict,orient='index',columns=['harmonic','spectrogram','mfcc','chroma','contrast'])\n",
    "\n",
    "train_df=ftrain\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbabeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While passing the Train Feature DataFrame, Classifier is throwing an error because of the value sequence\n",
    "#i.e. List for a single column value\n",
    "\n",
    "#Indexed columns for each set\n",
    "\n",
    "#Mel-Spectrogram\n",
    "train_spectrogram = pd.DataFrame(train_df.spectrogram.values.tolist(),index=train_df.index)\n",
    "train_spectrogram = train_spectrogram.add_prefix('mspec_')\n",
    "\n",
    "#MFCC\n",
    "train_cepstrum = pd.DataFrame(train_df.mfcc.values.tolist(),index=train_df.index)\n",
    "train_cepstrum = train_cepstrum.add_prefix('mfcc_')\n",
    "\n",
    "#ChromaEnergy\n",
    "train_chromaenergy = pd.DataFrame(train_df.chroma.values.tolist(),index=df_train.index)\n",
    "train_chromaenergy = train_chromaenergy.add_prefix('chroma_')\n",
    "\n",
    "#Contrast\n",
    "train_spectralcontrast = pd.DataFrame(train_df.contrast.values.tolist(), index=df_train.index)\n",
    "train_spectralcontrast = train_spectralcontrast.add_prefix('contrast-')\n",
    "\n",
    "#Chuck out the old columns\n",
    "train_df= train_df.drop(labels=['spectrogram','mfcc','chroma','contrast'],axis=1)\n",
    "\n",
    "#Concatenate the new indexed features with file name and tagets\n",
    "\n",
    "train_features = pd.concat([train_df,train_spectrogram,train_cepstrum,train_chromaenergy,train_spectralcontrast],axis=1, join='inner')\n",
    "train_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d80af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the target labels for training file\n",
    "#Running the loop over each name in train feature file \n",
    "\n",
    "targets_train=[]\n",
    "for name in train_features.index.tolist():\n",
    "    targets_train.append(instrument_class(name))\n",
    "\n",
    "#Adding a column of targets into the train feature file     \n",
    "train_features['targets']=targets_train\n",
    "train_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving training features with targets in a pickle file\n",
    "\n",
    "with open('/Users/divyansh/Downloads/Train-Test Data/train_features.pickle','wb') as f:\n",
    "    pickle.dump(train_features,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3556374",
   "metadata": {},
   "source": [
    "# Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to store all test features\n",
    "testdict={}\n",
    "\n",
    "#Loop over each file in test file \n",
    "#Test_audio being the directory to audio files in test dataset\n",
    "#Putting the features extracted into test dict\n",
    "\n",
    "for file in testfile:\n",
    "    features=features_extract(test_audio +file+ '.wav') #specifying directory and .wav\n",
    "    testdict[file]=features\n",
    "\n",
    "\n",
    "\n",
    "#Results of the specific audio files corresponding to json files in the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6978faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dict to DataFrame\n",
    "testfeatures=pd.DataFrame.from_dict(testdict, orient='index',columns=['harmonic','spectrogram','mfcc','chroma','contrast'])\n",
    "\n",
    "test_df=testfeatures\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b891ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DO THE SAME INDEXING AND CONCATENATION FOR TEST FEATURES\n",
    "\n",
    "#While passing the Test Feature DataFrame, Classifier is throwing an error because of the value sequence\n",
    "#Indexed columns for each set\n",
    "\n",
    "#Mel-Spectrogram\n",
    "spectrogram_test= pd.DataFrame(test_df.spectrogram.values.tolist(),index=test_df.index)\n",
    "spectrogram_test = spectrogram_test.add_prefix('mspec_')\n",
    "\n",
    "#MFCC\n",
    "mfcc_test= pd.DataFrame(test_df.mfcc.values.tolist(),index=test_df.index)\n",
    "mfcc_test=mfcc_test.add_prefix('mfcc_')\n",
    "\n",
    "#ChromaEnergy\n",
    "chroma_test= pd.DataFrame(test_df.chroma.values.tolist(),index=test_df.index)\n",
    "chroma_test= chroma_test.add_prefix('chroma_')\n",
    "\n",
    "#Contrast\n",
    "contrast_test= pd.DataFrame(test_df.contrast.values.tolist(), index=test_df.index)\n",
    "contrast_test= contrast_test.add_prefix('contrast-')\n",
    "\n",
    "#Chuck out the old columns\n",
    "test_df = test_df.drop(labels=['spectrogram','mfcc','chroma','contrast'],axis=1)\n",
    "\n",
    "#Concatenate the new indexed features with file name and tagets\n",
    "\n",
    "test_features= pd.concat([test_df, spectrogram_test, mfcc_test, chroma_test, contrast_test],axis=1, join='inner')\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af95332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Defining target variables for testing set\n",
    "\n",
    "targets_test= []\n",
    "\n",
    "for name in test_features.index.tolist():\n",
    "    targets_test.append(instrument_class(name))\n",
    "\n",
    "##Putting the target column in the test feature file\n",
    "test_features['targets']= targets_test\n",
    "test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features list for Test set in pickle file\n",
    "\n",
    "with open('/Users/divyansh/Downloads/Train-Test Data/test_features.pickle','wb') as f:\n",
    "    pickle.dump(test_features,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e1bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1627ab8",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d0543",
   "metadata": {},
   "source": [
    "### 1) [Enthought](https://www.youtube.com/watch?v=MhOdbtPhbLU) \n",
    "\n",
    "- Even though Librosa is quite an easy library to implement.I followed this guy to understand how to work on such signals to get interesting representations and plots.\n",
    "\n",
    "### 2) [Valerio Velardo - The Sound of AI](https://www.youtube.com/watch?v=WJI-17MNpdE)\n",
    "- I had a tough time trying to understand how MFCCs work computationally if we were to calculate it step by step using python, this guy had really cool explanations on his channel about Mel Spectrograms and MFCCs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f1c98",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Shortcoming of the Code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The Train file for NSynth was for about 25-26gb in size, so instead we used the valid dataset as a train set to train the model.\n",
    "\n",
    "\n",
    "\n",
    "- To test it on actual songs with good amount of runtime to figure out between each feature overlap, what all 'multiple' instruments are playing in the background.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Get live visualization results off playing an AudioClip about the pitch,amplitude etc, kind of how a Equaliser looks like in a music system. [Not edit the freq nobs ofcourse, just to feel of what's happening during the entire flow of song would have been interesting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12063e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08a632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37d57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
